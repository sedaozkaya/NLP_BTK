{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#metinlerde bulunan fazla boşlukları temizleme\n",
    "text = \"   hello    world!    2035   \"\n",
    "text.split()\n",
    "\n",
    "cleaned_text1 = \" \".join(text.split())\n",
    "print(text)\n",
    "print(cleaned_text1)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#noktalama isaretlerini temizleme\n",
    "import string\n",
    "text = \"Hello, World! 2035.\"\n",
    "cleaned_text3 = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(text)\n",
    "print(cleaned_text3)"
   ],
   "id": "f0bd4a96b58af2a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# buyuk kucuk harf cevirisi\n",
    "text = \"Hello, World! 2035\"\n",
    "cleaned_text2 = text.lower()\n",
    "print(text)\n",
    "print(cleaned_text2)"
   ],
   "id": "7621e0d740876a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#ozel karakterleri temizleme\n",
    "text = \"Hello, World! 2035 %&bb+^'@\"\n",
    "import re\n",
    "cleaned_text4 = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "print(text)\n",
    "print(cleaned_text4)"
   ],
   "id": "81095f0abda41892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#yazım hatalarını düzeltme\n",
    "from textblob import TextBlob #metin analizi icin kullanilan bir kutuphane\n",
    "text = \"Hellio , Wirld! 2035\"\n",
    "cleaned_text5 = str(TextBlob(text).correct())\n",
    "print(text)\n",
    "print(cleaned_text5)"
   ],
   "id": "ddab9f1ff1ccd320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#html ve url etiketlerini temizleme\n",
    "from bs4 import BeautifulSoup\n",
    "text = \"<html><h2>Hello, World! 2035</h2></html>\"\n",
    "cleaned_text6 = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "print(text) \n",
    "print(cleaned_text6)"
   ],
   "id": "3e007fab42200ea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#tokenizasyon\n",
    "import nltk\n",
    "\n",
    "text = \"hello, my name is John. I am a software engineer. I am from Turkey...\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)\n",
    "\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ],
   "id": "5255b92775642a82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T13:07:32.489335Z",
     "start_time": "2025-03-06T13:07:32.481844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#stemming ve lemmatization\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\") #lemmatization işlemi için gerekli veri tabanı\n",
    "from nltk.stem import PorterStemmer #stemming için fonksiyon\n",
    "#porterstemmer nesnesini oluştur\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"ran\", \"runs\", \"runner\" , \"better\" , \"go\", \"went\", \"gone\"]\n",
    "stems = [stemmer.stem(w) for w in words] #kelimelerin stemlerini bul. \n",
    "print(stems)\n",
    "\n",
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"ran\", \"runs\", \"runner\" , \"better\" , \"go\", \"went\", \"gone\"]\n",
    "lemmas = [lemmatizer.lemmatize(w , pos='v') for w in words]\n",
    "print(\"lemmas\")\n",
    "print(lemmas)"
   ],
   "id": "fdf45055b89f219b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'ran', 'run', 'runner', 'better', 'go', 'went', 'gone']\n",
      "lemmas\n",
      "['run', 'run', 'run', 'runner', 'better', 'go', 'go', 'go']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Seda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T13:21:35.235210Z",
     "start_time": "2025-03-06T13:21:35.222272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")#farklı dillerdeki stopwords veri tabanını indirir\n",
    "#stopwords-EN\n",
    "stop_words_en = set(stopwords.words(\"english\"))\n",
    "text = \"hello, my name is John. I am a software engineer. I am from Turkey. I am learning NLP.\"\n",
    "text_list = text.split()\n",
    "filtered_words =[w for w in text_list if w.lower() not in stop_words_en]\n",
    "print(\"filtered_words\")\n",
    "print(filtered_words)\n",
    "\n",
    "#stopwords-TR\n",
    "stop_words_tr = set(stopwords.words(\"turkish\"))\n",
    "text = \"merhaba, benim adım John. Ben bir yazılım mühendisiyim. Ben Türkiye'denim. NLP öğreniyorum. bu şu de mı \"\n",
    "text_list = text.split()\n",
    "filtered_words =[w for w in text_list if w.lower() not in stop_words_tr]\n",
    "print(\"filtered_words\")\n",
    "print(filtered_words)\n",
    "\n",
    "#kütüphanesiz stopwords temizleme\n",
    "\n",
    "tr_stopwords = [\"acaba\", \"ama\", \"aslında\", \"az\", \"bazı\", \"belki\", \"biri\", \"birkaç\", \"birşey\", \"biz\", \"bu\", \"çok\", \"çünkü\", \"da\", \"daha\", \"de\",]\n",
    "text = \"acaba bu cümledeki  birkaç gereksiz kelimeleri temizleyebilecek miyim\"\n",
    "text_list = text.split()\n",
    "filtered_words =[w for w in text_list if w.lower() not in tr_stopwords]\n",
    "print(\"filtered_words\")\n",
    "print(filtered_words)\n",
    "\n"
   ],
   "id": "315a9a092e95c5d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_words\n",
      "['hello,', 'name', 'John.', 'software', 'engineer.', 'Turkey.', 'learning', 'NLP.']\n",
      "filtered_words\n",
      "['merhaba,', 'benim', 'adım', 'John.', 'Ben', 'bir', 'yazılım', 'mühendisiyim.', 'Ben', \"Türkiye'denim.\", 'NLP', 'öğreniyorum.']\n",
      "filtered_words\n",
      "['cümledeki', 'gereksiz', 'kelimeleri', 'temizleyebilecek', 'miyim']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Seda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "da2b79093b282c2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
